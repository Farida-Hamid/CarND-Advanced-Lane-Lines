{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Lane Finding\n",
    "\n",
    "## Writeup Template\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Lane Finding Project\n",
    "##### The goals / steps of this project are the following:\n",
    "- Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "- Apply a distortion correction to raw images.\n",
    "- Use color transforms, gradients, etc., to create a threshold binary image.\n",
    "- Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "- Detect lane pixels and fit to find the lane boundary.\n",
    "- Determine the curvature of the lane and vehicle position with respect to center.\n",
    "- Warp the detected lane boundaries back onto the original image.\n",
    "- Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Rubric Points\n",
    "\n",
    "\n",
    "##### Here I will consider the rubric points individually and describe how I addressed each point in my implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera Calibration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 1) Briefly state how you computed the camera matrix and distortion coefficients. Provide an example of a distortion corrected calibration image.\n",
    "\n",
    "The code for this step is in the third code cell of the IPython notebook called pipeline.\n",
    "\n",
    "I start by preparing \"object points\", which will be the (x, y, z) coordinates of the chessboard corners in the world. Here I am assuming the chessboard is fixed on the (x, y) plane at z=0, such that the object points are the same for each calibration image. Thus, \"objp\" is just a replicated array of coordinates, and \"objpoints\" will be appended with a copy of it. \"imgpoints\" will be appended with the (x, y) pixel position of each of the corners in the image plane in case of a successful chessboard detection.\n",
    "\n",
    "I calculated \"dist\" and \"mtx\" for all the images and averaged their results.\n",
    "\n",
    "I then used the output \"objpoints\" and \"imgpoints\" to compute the camera calibration and distortion coefficients using the \"cv2.calibrateCamera()\" function. I applied this distortion correction to the test image using the \"cv2.undistort()\" function and obtained this result:\n",
    "\n",
    "<figure>\n",
    " <img src=\"report_images\\camcal.png\" width=\"961\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Camera calibration (above)</p> \n",
    " </figcaption>\n",
    "</figure>\n",
    " <p></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline (test images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Provide an example of a distortion-corrected image.\n",
    "\n",
    "Distortion correction that was calculated via camera calibration has been correctly applied to each image. An example of a distortion corrected image using the \"undistort\" function in the fourth code cell is:\n",
    "\n",
    "<figure>\n",
    " <img src=\"report_images\\undist.png\" width=\"961\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Example of an undistorted image (above)</p> \n",
    " </figcaption>\n",
    "</figure>\n",
    " <p></p> \n",
    "\n",
    "#### 2) Describe how (and identify where in your code) you used color transforms, gradients or other methods to create a threshold binary image. Provide an example of a binary image result.\n",
    "\n",
    "I used a combination of color filtering and gradient: for better recognition results.\n",
    "\n",
    "Using the \"color_selection\" function in the fourth code cell, yellow and white pixels were selected from the image, then the image was threshold. This worked well for most images but not all of them.\n",
    "\n",
    "*Note:* The following results are for warped images, since better results were obtained on warped images.\n",
    "\n",
    "\n",
    "<figure>\n",
    " <img src=\"report_images\\colfil.png\" width=\"961\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Example of a *successful* color filtered image (above)</p> \n",
    " </figcaption>\n",
    "</figure>\n",
    " <p></p> \n",
    " \n",
    "<figure>\n",
    " <img src=\"report_images\\badcolfil.png\" width=\"961\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Example of an *unsuccessful* color filtered image (above)</p> \n",
    " </figcaption>\n",
    "</figure>\n",
    " <p></p> \n",
    " \n",
    "Using LUV and HLS color spaces did not help much as it mostly failed where color selection failed.\n",
    "\n",
    "<figure>\n",
    " <img src=\"report_images\\HLS.png\" width=\"961\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Example of S channel in HLS color space (above)</p> \n",
    " </figcaption>\n",
    "</figure>\n",
    " <p></p> \n",
    " \n",
    "Also, images were threshold using the \"in_sobel\" function in the fourth code cell. Better results were obtained when calculating the threshold image before warping it, unlike color filtering.\n",
    "\n",
    "<figure>\n",
    " <img src=\"report_images\\sbl.png\" width=\"961\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Example of a *successful* threshold image (above)</p> \n",
    " </figcaption>\n",
    "</figure>\n",
    " <p></p> \n",
    " \n",
    "<figure>\n",
    " <img src=\"report_images\\badsbl.png\" width=\"961\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Example of an *unsuccessful* threshold image (above)</p> \n",
    " </figcaption>\n",
    "</figure>\n",
    " <p></p> \n",
    "\n",
    "The results of the two methods were added together before being fed to the line detection function.\n",
    "\n",
    "The following images are examples where one of the two methods failed when the other didn't. \n",
    "<figure>\n",
    " <img src=\"report_images\\cl1.png\" width=\"961\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Color filtered images in the first column, sobeled images in the second column and the summation of both in the third column</p> \n",
    " </figcaption>\n",
    "</figure>\n",
    " <p></p> \n",
    "\n",
    "#### 3) Describe how (and identify where in your code) you performed a perspective transform and provide an example of a transformed image.\n",
    "\n",
    "Using the \"warper\" function in the fourth code cell was used to correctly rectify each image to a birds-eye view. The following table shows the source and destination points used to perform perspective transform:\n",
    "\n",
    "\n",
    "|Position    |Source points| Destination points|\n",
    "|------------|-------------|-------------------|\n",
    "|Bottom right| [1310,720]  |    [1100,720]     |\n",
    "|Bottom left |  [100,720]  |     [100,720]     |\n",
    "|Top right   |  [717, 450] |      [1200,0]     |\n",
    "|Top left    |  [580,450]  |       [80,0]      |\n",
    "\n",
    "\n",
    "Here's an example of a warped image:\n",
    "\n",
    "<figure>\n",
    " <img src=\"report_images\\warp.png\" width=\"961\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Original and undistorted images (above)</p> \n",
    " </figcaption>\n",
    "</figure>\n",
    " <p></p> \n",
    " \n",
    "#### 4) Describe how (and identify where in your code) you identified lane-line pixels and fit their positions with a polynomial?\n",
    "\n",
    "The \"lines\" function in the fifth code cell has been used to identify lane line pixels in the rectified binary image. The left and right line have been identified and fit with a curved functional form (e.g., polynomial). The example bellow shows the detected lines in yellow, right line in blue and the left one in red.\n",
    "\n",
    "<figure>\n",
    " <img src=\"report_images\\line.png\" width=\"461\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Example of final result (above)</p> \n",
    " </figcaption>\n",
    "</figure>\n",
    " <p></p> \n",
    " \n",
    "\n",
    "#### 5) Describe how (and identify where in your code) you calculated the radius of curvature of the lane and the position of the vehicle with respect to center.\n",
    "\n",
    "Lines 90-95 in the \"lines\" function are used to calculate the curvature. Lines 89-100 calculate the position of the car with respect to the center.\n",
    "\n",
    "#### 6) Provide an example image of your result plotted back down onto the road such that the lane area is identified clearly.\n",
    "\n",
    "The fit from the rectified image has been warped back onto the original image and plotted to identify the lane boundaries. The lane boundaries were correctly identified. An example image with lanes, curvature, and position from center is shown below.\n",
    "\n",
    "<figure>\n",
    " <img src=\"report_images\\final.png\" width=\"461\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Example of final result (above)</p> \n",
    " </figcaption>\n",
    "</figure>\n",
    " <p></p> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline (video)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline used to process the video is shown in the sixth code cell. The radius of curvature of the lane and vehicle position within the lane are shown in the video.\n",
    "\n",
    "The output video is called *final.mp4* and is included in the project folder. The final result is well acceptable; the lane is identified despite the changes in the pavement and shadows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest problem for this project is the unpredictable colors of pavement, light conditions and environment.\n",
    "\n",
    "The lane shown in green is very well detected except for the end of the detected area when driving through the pavement with lighter color. Yet the result is well acceptable.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
